<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Ollama on Wayne&#39;Log</title>
    <link>http://localhost:12409/tags/ollama/</link>
    <description>Recent content in Ollama on Wayne&#39;Log</description>
    <generator>Hugo -- 0.155.2</generator>
    <language>en-US</language>
    <lastBuildDate>Fri, 21 Feb 2025 13:02:59 +0800</lastBuildDate>
    <atom:link href="http://localhost:12409/tags/ollama/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>通过公网IP调用暴露的Ollama服务</title>
      <link>http://localhost:12409/zh/posts/2025_02_21-using_public_ip_to_freely_access_ollama/</link>
      <pubDate>Fri, 21 Feb 2025 13:02:59 +0800</pubDate>
      <guid>http://localhost:12409/zh/posts/2025_02_21-using_public_ip_to_freely_access_ollama/</guid>
      <description>&lt;blockquote class=&#34;quote&#34;&gt;&lt;p&gt;如何使用ollama-这是一个系列&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;../2025_02_09-ollama_deepseek_1&#34;
   
   
   class=&#34;custom-link&#34;&gt;  
   通过Ollama调用DeepSeek&lt;/a&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;.&#34;
   
   
   class=&#34;custom-link&#34;&gt;  
   通过公网IP调用暴露的Ollama服务&lt;/a&gt;
&amp;#x1f448; 你在这里&lt;/li&gt;
&lt;/ul&gt;&lt;/blockquote&gt;

&lt;div class=&#34;admonition warning &#34;&gt;
  &lt;div class=&#34;admonition-header&#34;&gt;
    &lt;i class=&#34;fas fa-exclamation-triangle&#34;&gt;&lt;/i&gt;
    &lt;span&gt;Warning&lt;/span&gt;
    
  &lt;/div&gt;
  &lt;div class=&#34;admonition-content&#34;&gt;
    使用这种方法时，请务必尊重服务器的隐私和使用政策，避免滥用公共资源。
  &lt;/div&gt;
&lt;/div&gt;


&lt;p&gt;随着 DeepSeek 模型的广泛应用，越来越多的人开始在自己的服务器上运行 Ollama 平台来本地执行模型。由于 Ollama 在默认配置下不需要 API 密钥，只需要知道服务器的 IP 地址和端口，就能连接并访问正在运行的模型。因此，如果你能够找到这些暴露在公网的 Ollama 实例，就能免费使用它们。&lt;/p&gt;</description>
      <content:encoded><![CDATA[<blockquote class="quote"><p>如何使用ollama-这是一个系列</p>
<ul>
<li><a href="../2025_02_09-ollama_deepseek_1"
   
   
   class="custom-link">  
   通过Ollama调用DeepSeek</a>
</li>
<li><a href="."
   
   
   class="custom-link">  
   通过公网IP调用暴露的Ollama服务</a>
&#x1f448; 你在这里</li>
</ul></blockquote>

<div class="admonition warning ">
  <div class="admonition-header">
    <i class="fas fa-exclamation-triangle"></i>
    <span>Warning</span>
    
  </div>
  <div class="admonition-content">
    使用这种方法时，请务必尊重服务器的隐私和使用政策，避免滥用公共资源。
  </div>
</div>


<p>随着 DeepSeek 模型的广泛应用，越来越多的人开始在自己的服务器上运行 Ollama 平台来本地执行模型。由于 Ollama 在默认配置下不需要 API 密钥，只需要知道服务器的 IP 地址和端口，就能连接并访问正在运行的模型。因此，如果你能够找到这些暴露在公网的 Ollama 实例，就能免费使用它们。</p>
<p>本文将介绍如何利用公网 IP 精准访问 Ollama 服务器，并使用其中的 AI 模型。</p>
<h2 id="原理概述">原理概述</h2>
<p>Ollama 是一个支持本地运行 AI 模型的平台，在默认配置下，Ollama 不需要身份验证或 API 密钥，只要知道服务器的 IP 地址和端口，就能直接连接并访问其运行的模型。随着 DeepSeek 模型的火爆，许多服务器开始运行 Ollama 来执行这些模型。</p>
<p>通过简单的 GET 请求，你可以查看服务器上正在运行的模型，确认它们是否符合你的需求。这就使得你能够在没有任何授权的情况下，利用开放的服务器资源，进行“白嫖”。</p>
<h2 id="实施步骤">实施步骤</h2>
<h3 id="使用-fofa-查找-ollama-实例">使用 Fofa 查找 Ollama 实例</h3>
<p>首先，你需要通过搜索引擎如 Fofa 来查找暴露了 Ollama 服务的公网 IP 地址。你可以使用以下搜索语句：</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-text-size-adjust:none;"><code class="language-fallback" data-lang="fallback"><span style="display:flex;"><span>app=&#34;Ollama&#34; &amp;&amp; is_domain=false
</span></span></code></pre></div><p>这个查询帮助你定位那些运行在端口 11434 上的 Ollama 实例。这些实例通常没有做过多的访问限制，只要你知道地址，就能连接到它们。</p>
<h3 id="导出并验证可用的服务">导出并验证可用的服务</h3>
<p>找到目标服务器后，你可以导出相关数据，并使用批量访问脚本进行验证。你可以自己编写脚本，也可以使用 Burp Suite 等工具进行暴力破解，检查哪些服务器开放了能够使用的接口。</p>
<h3 id="查看正在运行的模型">查看正在运行的模型</h3>
<p>一旦成功连接到服务器，你可以通过发送 GET 请求来查看服务器上运行的模型。具体地址为：</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-text-size-adjust:none;"><code class="language-fallback" data-lang="fallback"><span style="display:flex;"><span>URL:/api/ps
</span></span></code></pre></div><p>该接口会返回服务器当前运行的所有模型的信息。如果你看到 DeepSeek 模型的名称，并且标明了参数数量（如 70B、32B），那么说明这个模型可以访问。</p>
<h3 id="选择合适的-ai-软件进行交互">选择合适的 AI 软件进行交互</h3>
<p>确认服务器上运行的模型后，你可以选择适合的 AI 软件来调用 Ollama 提供的 API，与模型进行交互。例如，70B 版本的模型适合处理大规模的计算任务，而 32B 版本则适合轻量级应用。</p>
<h3 id="测试和验证模型的可用性">测试和验证模型的可用性</h3>
<p>通过多次测试，你可以发现哪些模型可用，哪些模型由于服务器限制或负载过高可能无法访问。一般来说，70B 和 32B 是最常见的有效模型，而如 671B 等大规模模型较为罕见。</p>
<h2 id="总结">总结</h2>
<p>通过这一方法，你可以在没有 API 密钥的情况下，精准地连接到正在运行 Ollama 的服务器，并使用其中的 DeepSeek 模型。只要知道目标服务器的地址和端口，你就能通过简单的 GET 请求，查看并使用这些开放的 AI 模型。这种方法被称为“精准白嫖”，它依赖于大规模使用 Ollama 的服务器的暴露服务。</p>
]]></content:encoded>
    </item>
    <item>
      <title>通过Ollama调用DeepSeek</title>
      <link>http://localhost:12409/zh/posts/2025_02_09-ollama_deepseek_1/</link>
      <pubDate>Sun, 09 Feb 2025 20:08:43 +0800</pubDate>
      <guid>http://localhost:12409/zh/posts/2025_02_09-ollama_deepseek_1/</guid>
      <description>&lt;blockquote class=&#34;quote&#34;&gt;&lt;p&gt;如何使用ollama-这是一个系列&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;.&#34;
   
   
   class=&#34;custom-link&#34;&gt;  
   通过Ollama调用DeepSeek&lt;/a&gt;
&amp;#x1f448; 你在这里&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;../2025_02_21-using_public_ip_to_freely_access_ollama&#34;
   
   
   class=&#34;custom-link&#34;&gt;  
   通过公网IP调用暴露的Ollama服务&lt;/a&gt;
&lt;/li&gt;
&lt;/ul&gt;&lt;/blockquote&gt;

&lt;h2 id=&#34;ollama&#34;&gt;Ollama&lt;/h2&gt;
&lt;p&gt;&lt;blockquote class=&#34;quote&#34;&gt;&lt;p&gt;&amp;ldquo;Get up and running with large language models locally.&amp;rdquo;&lt;/p&gt;&lt;/blockquote&gt;

想必大家一定从很多地方都看到过这个一直小羊驼&amp;ndash;&lt;a href=&#34;https://github.com/ollama/ollama&#34;
   
    
       target=&#34;_blank&#34; rel=&#34;noopener&#34; 
   
   class=&#34;custom-link&#34;&gt;  
   Ollama&lt;span class=&#34;external-link&#34;&gt;↗&lt;/span&gt;&lt;/a&gt;
，正如官方仓库所言，Ollama旨在简化大语言模型(LLMs)的本地部署和使用，我们能够通过这个这个工具来实现轻松下载、运行和管理各种开源的大语言模型。&lt;/p&gt;</description>
      <content:encoded><![CDATA[<blockquote class="quote"><p>如何使用ollama-这是一个系列</p>
<ul>
<li><a href="."
   
   
   class="custom-link">  
   通过Ollama调用DeepSeek</a>
&#x1f448; 你在这里</li>
<li><a href="../2025_02_21-using_public_ip_to_freely_access_ollama"
   
   
   class="custom-link">  
   通过公网IP调用暴露的Ollama服务</a>
</li>
</ul></blockquote>

<h2 id="ollama">Ollama</h2>
<p><blockquote class="quote"><p>&ldquo;Get up and running with large language models locally.&rdquo;</p></blockquote>

想必大家一定从很多地方都看到过这个一直小羊驼&ndash;<a href="https://github.com/ollama/ollama"
   
    
       target="_blank" rel="noopener" 
   
   class="custom-link">  
   Ollama<span class="external-link">↗</span></a>
，正如官方仓库所言，Ollama旨在简化大语言模型(LLMs)的本地部署和使用，我们能够通过这个这个工具来实现轻松下载、运行和管理各种开源的大语言模型。</p>
<ul>
<li>Ollama支持的模型仓库：<a href="https://ollama.com/library"
   
    
       target="_blank" rel="noopener" 
   
   class="custom-link">  
   https://ollama.com/library<span class="external-link">↗</span></a>
</li>
</ul>
<p>我在这里使用的是 deepseek-r1:14b，因为我的笔记本没有 GPU，但 Ollama 支持在没有 GPU 的情况下调用 CPU 来运行模型，所以也能够正常运行。</p>
<blockquote>
<p>注意：运行 7B 模型至少需要 8GB 内存，运行 14B 模型至少需要 16GB 内存，运行 33B 模型至少需要 32GB 内存。</p>
</blockquote>
<h2 id="deepseek">DeepSeek</h2>
<p><blockquote class="quote"><p>&ldquo;DeepSeek’s first-generation reasoning models, achieving performance comparable to OpenAI-o1 across math, code, and reasoning tasks.&rdquo;</p></blockquote>

<a href="https://www.deepseek.com/"
   
    
       target="_blank" rel="noopener" 
   
   class="custom-link">  
   DeepSeek<span class="external-link">↗</span></a>
<span class="sidenote-number"><small class="sidenote"><a href="https://github.com/deepseek-ai/DeepSeek-V3"
   
    
       target="_blank" rel="noopener" 
   
   class="custom-link">  
   DeepSeek-V3<span class="external-link">↗</span></a>
是其最新的开源模型项目，完整模型为671B，<a href="https://github.com/deepseek-ai/DeepSeek-V3/blob/main/DeepSeek_V3.pdf"
   
    
       target="_blank" rel="noopener" 
   
   class="custom-link">  
   论文链接<span class="external-link">↗</span></a></small></span>

（深度求索）是中国人工智能公司深度求索（DeepSeek Inc.）开发的一系列开源大语言模型（LLM），专注于高效推理和低成本部署。其中的DeepSeek-R1是其第一代推理模型，在推理任务上的表现与OpenAI-o1相当，同时为了支持研究社区，DeepSeek开源了DeepSeek-R1以及基于Qwen2.5和Llama3蒸馏出来的共记7个模型。</p>
<h2 id="安装ollama">安装Ollama</h2>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-text-size-adjust:none;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>$ sudo pacman -S ollama  
</span></span></code></pre></div><p>官方推荐的方式为如下：</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-text-size-adjust:none;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>$ curl -fsSL https://ollama.com/install.sh | sh  
</span></span></code></pre></div><h2 id="下拉deepseek模型">下拉DeepSeek模型</h2>
<p>Ollama现在已经将deepseek模型接入官方库中，我们只需要通过以下命令拉取模型即可：</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-text-size-adjust:none;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>$ ollama pull deepseek-r1:14b
</span></span></code></pre></div><h2 id="通过ollama启动deepseek">通过Ollama启动DeepSeek</h2>
<p>经过上述部分，我们已经可以尝试本机运行DeepSeek了。通过以下命令启动Ollama服务：</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-text-size-adjust:none;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>$ ollama server  
</span></span></code></pre></div><p>在启动Ollama服务过后，我们即可使用以下命令来尝试DeepSeek了：</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-text-size-adjust:none;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>$ ollama run deepseek-r1:14b
</span></span></code></pre></div><p>运行示例如下：</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-text-size-adjust:none;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>&gt;&gt;&gt; which is greater? 9.11 or 9.9
</span></span><span style="display:flex;"><span>&lt;think&gt;
</span></span><span style="display:flex;"><span>First, I observe that both numbers have the same whole number part, which is 9.
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>Next, I compare their decimal parts: 0.11 and 0.90.
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>Since 0.11 is less than 0.90, it follows that 9.11 is less than 9.9.
</span></span><span style="display:flex;"><span>&lt;/think&gt;
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>To determine which number is greater between **9.11** and **9.9**, let&#39;s compare them step 
</span></span><span style="display:flex;"><span>by step.
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#6272a4">### Step 1: Understand the Numbers</span>
</span></span><span style="display:flex;"><span>- **9.11** can be written as <span style="color:#f1fa8c">\(</span><span style="color:#bd93f9">9</span> + <span style="color:#f1fa8c">\f</span>rac<span style="color:#ff79c6">{</span>11<span style="color:#ff79c6">}{</span>100<span style="color:#ff79c6">}</span><span style="color:#f1fa8c">\)</span>
</span></span><span style="display:flex;"><span>- **9.9** can be written as <span style="color:#f1fa8c">\(</span><span style="color:#bd93f9">9</span> + <span style="color:#f1fa8c">\f</span>rac<span style="color:#ff79c6">{</span>9<span style="color:#ff79c6">}{</span>10<span style="color:#ff79c6">}</span><span style="color:#f1fa8c">\)</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#6272a4">### Step 2: Compare the Decimal Parts</span>
</span></span><span style="display:flex;"><span>- The decimal part of **9.11** is **0.11**
</span></span><span style="display:flex;"><span>- The decimal part of **9.9** is **0.90**
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>Now, compare **0.11** and **0.90**:
</span></span><span style="display:flex;"><span><span style="color:#f1fa8c">\[</span> 0.11 &lt; 0.90 <span style="color:#f1fa8c">\]</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#6272a4">### Step 3: Conclusion</span>
</span></span><span style="display:flex;"><span>Since the decimal part of **9.11** is less than that of **9.9**, it follows that:
</span></span><span style="display:flex;"><span><span style="color:#f1fa8c">\[</span> 9.11 &lt; 9.9 <span style="color:#f1fa8c">\]</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>Therefore, **9.9** is greater than **9.11**.
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#f1fa8c">\[</span>
</span></span><span style="display:flex;"><span><span style="color:#f1fa8c">\b</span>oxed<span style="color:#ff79c6">{</span>9.9<span style="color:#ff79c6">}</span>
</span></span><span style="display:flex;"><span><span style="color:#f1fa8c">\]</span>
</span></span></code></pre></div><h2 id="ollama-server-自启动">Ollama Server 自启动</h2>
<p>为了在开机时自启 Ollama Server，我们可以使用 systemd 来管理自动启动：</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-text-size-adjust:none;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>$ sudo vim /etc/systemd/system/ollama-server.service
</span></span></code></pre></div><p>我们在其中填入以下内容：<br>















  
    
  

  
    
  

  
    
  

  
    
  

  
    
  

  
    
  

  
    
  

  
    
  

  
    
  

  
    
  

  
    
  

  
    
  

  
    
  









<p><details class="custom-collapse" open>
  <summary markdown="span">
    <span>/etc/systemd/system/ollama-server.service</span>
    <span class="line-count">12 lines</span>
  </summary>
  <div class="content">
    <div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-text-size-adjust:none;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span><span style="color:#ff79c6">[</span>Unit<span style="color:#ff79c6">]</span>
</span></span><span style="display:flex;"><span><span style="color:#8be9fd;font-style:italic">Description</span><span style="color:#ff79c6">=</span>Ollama Server
</span></span><span style="display:flex;"><span><span style="color:#8be9fd;font-style:italic">After</span><span style="color:#ff79c6">=</span>network.target
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#ff79c6">[</span>Service<span style="color:#ff79c6">]</span>
</span></span><span style="display:flex;"><span><span style="color:#8be9fd;font-style:italic">ExecStart</span><span style="color:#ff79c6">=</span>ollama serve
</span></span><span style="display:flex;"><span><span style="color:#8be9fd;font-style:italic">Restart</span><span style="color:#ff79c6">=</span>on-failure
</span></span><span style="display:flex;"><span><span style="color:#8be9fd;font-style:italic">RestartSec</span><span style="color:#ff79c6">=</span><span style="color:#bd93f9">5</span>
</span></span><span style="display:flex;"><span><span style="color:#8be9fd;font-style:italic">Environment</span><span style="color:#ff79c6">=</span><span style="color:#8be9fd;font-style:italic">HOME</span><span style="color:#ff79c6">=</span>/home/&lt;your home name&gt;
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#ff79c6">[</span>Install<span style="color:#ff79c6">]</span>
</span></span><span style="display:flex;"><span><span style="color:#8be9fd;font-style:italic">WantedBy</span><span style="color:#ff79c6">=</span>multi-user.target
</span></span></code></pre></div>
  </div>
</details></p>

创建完服务文件后，通过以下指令来完成 systemd 配置：</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-text-size-adjust:none;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span><span style="color:#6272a4"># 重新加载 systemd 配置</span>
</span></span><span style="display:flex;"><span>$ sudo systemctl daemon-reload
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#6272a4"># 下次开机后，启动开机自启</span>
</span></span><span style="display:flex;"><span>$ sudo systemctl <span style="color:#8be9fd;font-style:italic">enable</span> ollama-server.service
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#6272a4"># 立刻启动服务</span>
</span></span><span style="display:flex;"><span>$ sudo systemctl start ollama-server.service
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#6272a4"># 查看服务状态</span>
</span></span><span style="display:flex;"><span>$ sudo systemctl status ollama-server.service
</span></span></code></pre></div><h2 id="通过open-webui-调用本地的deepseek-api">通过Open-webui 调用本地的DeepSeek api</h2>
<p>TODO</p>
]]></content:encoded>
    </item>
  </channel>
</rss>
